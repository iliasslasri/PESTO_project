{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad28af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import glob\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "from pesto import load_model\n",
    "import mir_eval\n",
    "from mir1k_dataset import MIR1KDataset\n",
    "from torch.utils.data import Dataset\n",
    "import mirdata\n",
    "import pandas as pd\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6871d8a5",
   "metadata": {},
   "source": [
    "#### EVAL ON MIR-1K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bacdeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIR1KDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, mix=0.0, target_sr=16000):\n",
    "        self.wav_files = sorted(glob.glob(os.path.join(root, \"Wavfile\", \"*.wav\")))\n",
    "        self.pitch_dir = os.path.join(root, \"PitchLabel\")\n",
    "        self.mix = mix\n",
    "        self.target_sr = target_sr\n",
    "        self.resampler_cache = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wav_files)\n",
    "\n",
    "    def _get_resampler(self, sr):\n",
    "        if sr not in self.resampler_cache:\n",
    "            self.resampler_cache[sr] = T.Resample(sr, self.target_sr)\n",
    "        return self.resampler_cache[sr]\n",
    "\n",
    "    def generate_rir(self, sr, duration=1.0):\n",
    "        # Generate simple synthetic RIR (exponential decay noise)\n",
    "        t = torch.linspace(0, duration, int(duration * sr))\n",
    "        envelope = torch.exp(-5.0 * t)\n",
    "        noise = torch.randn_like(t)\n",
    "        rir = noise * envelope\n",
    "        return rir / torch.norm(rir)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_path = self.wav_files[idx]\n",
    "        basename = os.path.basename(wav_path)\n",
    "        pv_name = basename.replace(\".wav\", \".pv\")\n",
    "        pv_path = os.path.join(self.pitch_dir, pv_name)\n",
    "\n",
    "        x, sr = torchaudio.load(wav_path)\n",
    "        x = x[1:2, :] # take vocals only\n",
    "        if sr != self.target_sr:\n",
    "            x = self._get_resampler(sr)(x)\n",
    "\n",
    "        # Apply Reverb\n",
    "        if self.mix > 0:\n",
    "            rir = self.generate_rir(self.target_sr).to(x.device).unsqueeze(0) # [1, T_rir]\n",
    "            # Convolve\n",
    "            rev = F.fftconvolve(x, rir, mode=\"full\")\n",
    "            rev = rev[:, :x.shape[1]]  # Keep original length\n",
    "            \n",
    "            # Normalize energy\n",
    "            x_norm = torch.norm(x)\n",
    "            rev_norm = torch.norm(rev)\n",
    "            if rev_norm > 0:\n",
    "                rev = rev * (x_norm / rev_norm)\n",
    "            \n",
    "            x = (1 - self.mix) * x + self.mix * rev\n",
    "\n",
    "        # Load Labels (MIDI Semitones)\n",
    "        # 0 = Unvoiced, >0 = Voiced Pitch\n",
    "        if os.path.exists(pv_path):\n",
    "            labels = np.loadtxt(pv_path)\n",
    "            # We convert MIDI labels to Hz for consistent evaluation.\n",
    "            # Hz = 440 * 2^((d-69)/12)\n",
    "            # Mask unvoiced (0) to avoid log/exp errors\n",
    "            mask = labels > 0\n",
    "            labels_hz = np.zeros_like(labels)\n",
    "            labels_hz[mask] = 440.0 * (2.0 ** ((labels[mask] - 69.0) / 12.0))\n",
    "        else:\n",
    "            labels_hz = np.array([])\n",
    "\n",
    "        return x, labels_hz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709549d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOP_SIZE_SECONDS = 0.020\n",
    "MIR_1K_PATH = \"./MIR-1K\"\n",
    "pesto_model = load_model(\n",
    "    'mir-1k_g7',\n",
    "    step_size=20.,\n",
    "    sampling_rate=16000, # mir-1k is in sampled @16k\n",
    "    max_batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddc4cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(dataset_root):\n",
    "    \n",
    "    mix_levels = [0.0, 0.1, 0.2, 0.3, 0.6, 0.9]\n",
    "    \n",
    "    for mix in mix_levels:\n",
    "        print(f\"\\n--- Evaluating Reverb Mix: {mix} ---\")\n",
    "\n",
    "        dataset = MIR1KDataset(dataset_root, mix=mix, target_sr=16000)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "        \n",
    "        # Accumulators for metrics\n",
    "        total_rpa = 0.0\n",
    "        total_rca = 0.0\n",
    "        count = 0\n",
    "        metrics = {}\n",
    "        metrics[\"RPA\"] = 0\n",
    "        metrics[\"RCA\"] = 0\n",
    "        metrics[\"OA\"] = 0\n",
    "        for batch in tqdm(loader):\n",
    "            x, y = batch\n",
    "            # x: [1, 1, T], y: [1, L]\n",
    "            \n",
    "            audio_input = x.squeeze(0) # [1, T]\n",
    "            if count==0:\n",
    "                sf.write(f\"audio_{mix}.wav\", audio_input[0].detach().cpu().numpy(),  samplerate=16000)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Get Pitch (Hz) and Confidence\n",
    "                pitch, conf, _ = pesto_model(audio_input, convert_to_freq=True, return_activations=False)\n",
    "            import ipdb\n",
    "            # --- Post-Processing ---\n",
    "            pred_hz = pitch.cpu().numpy().flatten()\n",
    "            conf_np = conf.cpu().numpy().flatten()\n",
    "            ref_hz = y.numpy().flatten()\n",
    "\n",
    "            # Apply Voicing Threshold\n",
    "            # Set unconfident predictions to 0 (Unvoiced)\n",
    "            est_freq = pred_hz.copy()\n",
    "            # est_freq[conf_np < 0.2] = 0.0\n",
    "            \n",
    "            # Generate Timestamps\n",
    "            # We construct time arrays so mir_eval knows exactly where each frame sits.\n",
    "            est_time = np.arange(len(est_freq)) * HOP_SIZE_SECONDS\n",
    "            ref_time = np.arange(len(ref_hz)) * HOP_SIZE_SECONDS\n",
    "\n",
    "            # mir_eval handles the interpolation/alignment automatically based on the timestamps\n",
    "            # We skip files where ground truth is empty or invalid\n",
    "            if len(ref_hz) > 0 and np.sum(ref_hz) > 0:\n",
    "                scores = mir_eval.melody.evaluate(ref_time, ref_hz, est_time, est_freq)\n",
    "                total_rpa += scores['Raw Pitch Accuracy']\n",
    "                total_rca += scores['Raw Chroma Accuracy']\n",
    "                count += 1\n",
    "\n",
    "        if count > 0:\n",
    "            avg_rpa = total_rpa / count\n",
    "            avg_rca = total_rca / count\n",
    "            print(f\"Result (Mix {mix}): RPA={avg_rpa*100:.2f}% | RCA={avg_rca*100:.2f}%\")\n",
    "        else:\n",
    "            print(\"No valid samples evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82942574",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(MIR_1K_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e118cb1c",
   "metadata": {},
   "source": [
    "### EVAL ON MDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba27bd",
   "metadata": {},
   "source": [
    "The dataset will be automatically downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0684a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDBDataset(Dataset):\n",
    "    def __init__(self, dataset_name: str, mix : float = 0.0):\n",
    "        # Initialize the loader, download if required, and validate\n",
    "        self.loader = mirdata.initialize(dataset_name)\n",
    "        self.loader.download()\n",
    "        self.loader.validate()\n",
    "        \n",
    "        # batch size must be 1 because here we do not pad the items\n",
    "        self.mix = mix\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.loader.track_ids)\n",
    "\n",
    "    def generate_rir(self, sr, duration=1.0):\n",
    "        # Generate simple synthetic RIR (exponential decay noise)\n",
    "        t = torch.linspace(0, duration, int(duration * sr))\n",
    "        envelope = torch.exp(-5.0 * t)\n",
    "        noise = torch.randn_like(t)\n",
    "        rir = noise * envelope\n",
    "        return (rir / torch.norm(rir)).numpy()\n",
    "\n",
    "    def __getitem__(self, item: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        # Unpack the current track\n",
    "        track_id = self.loader.track_ids[item]\n",
    "        track = self.loader.track(track_id)\n",
    "\n",
    "        # Get the audio and annotations\n",
    "        audio_signal, sample_rate = track.audio\n",
    "        audio_signal = audio_signal.mean(axis=-1) if audio_signal.ndim > 1 else audio_signal\n",
    "\n",
    "        if self.mix > 0:\n",
    "            rir = self.generate_rir(sample_rate)\n",
    "            # Convolve\n",
    "            rev = signal.fftconvolve(audio_signal, rir, mode=\"full\")\n",
    "            rev = rev[:audio_signal.shape[0]]  # Keep original length\n",
    "            \n",
    "            # Normalize energy\n",
    "            audio_signal_norm = np.linalg.norm(audio_signal)\n",
    "            rev_norm = np.linalg.norm(rev)\n",
    "            if rev_norm > 0:\n",
    "                rev = rev * (audio_signal_norm / rev_norm)\n",
    "            \n",
    "            audio_signal = (1 - self.mix) * audio_signal + self.mix * rev\n",
    "\n",
    "        times = track.f0.times\n",
    "        frequencies = track.f0.frequencies\n",
    "\n",
    "        return (\n",
    "            audio_signal.astype(np.float32),\n",
    "            times.astype(np.float32),\n",
    "            frequencies.astype(np.float32),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0f5439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_2():\n",
    "    fs = 44100\n",
    "    hpsz = 20 # ms\n",
    "    pesto_model = load_model(\"mir-1k_g7\", step_size=hpsz, sampling_rate=fs)\n",
    "    pesto_model.eval()\n",
    "    mix_levels = [0.0, 0.1, 0.2, 0.3, 0.6, 0.9]\n",
    "\n",
    "    metrics = {}\n",
    "    for mix in mix_levels:\n",
    "        count = 0\n",
    "        total_rpa = 0\n",
    "        total_rca = 0\n",
    "        total_oa = 0\n",
    "        metrics[mix] = {}\n",
    "        metrics[mix][\"RPA\"] = 0\n",
    "        metrics[mix][\"RCA\"] = 0\n",
    "        metrics[mix][\"OA\"] = 0\n",
    "        print(f\"\\n--- Evaluating Reverb Mix: {mix} ---\")\n",
    "        md = torch.utils.data.DataLoader(MDBDataset(\"mdb_stem_synth\", mix=mix), batch_size=1, shuffle=True, drop_last=False)\n",
    "        for audio, times, freqs in tqdm(md):\n",
    "            with torch.no_grad():\n",
    "                f0_pred, _, _ = pesto_model(\n",
    "                audio,\n",
    "                convert_to_freq=True,\n",
    "                return_activations=False,\n",
    "            )\n",
    "            f0_pred = np.nan_to_num(f0_pred, nan=0.0)\n",
    "\n",
    "            times_pred = np.arange(f0_pred.shape[-1]) * (hpsz / 1000.0)\n",
    "            times_pred = times_pred.flatten()\n",
    "            f0_pred = f0_pred.flatten()\n",
    "            times = times.numpy().flatten()\n",
    "            freqs = freqs.numpy().flatten()\n",
    "            scores = mir_eval.melody.evaluate(times, freqs, times_pred, f0_pred)\n",
    "            total_rpa += scores['Raw Pitch Accuracy']\n",
    "            total_rca += scores['Raw Chroma Accuracy']\n",
    "            total_oa += scores['Overall Accuracy']\n",
    "            count += 1\n",
    "        if count > 0:\n",
    "            avg_rpa = total_rpa / count\n",
    "            avg_rca = total_rca / count\n",
    "            avg_oa = total_oa / count\n",
    "            print(f\"Result (Mix {mix}): RPA={avg_rpa*100:.2f}% | RCA={avg_rca*100:.2f}%\")\n",
    "            metrics[mix][\"RPA\"] = avg_rpa*100\n",
    "            metrics[mix][\"RCA\"] = avg_rca*100\n",
    "            metrics[mix][\"OA\"] = avg_oa*100\n",
    "        else:\n",
    "            print(\"No valid samples evaluated.\")\n",
    "\n",
    "    df = pd.DataFrame.from_dict(metrics, orient='index')\n",
    "    df.index.name = 'Mix Level'\n",
    "    \n",
    "    df.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041714ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation_2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pesto-full",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
